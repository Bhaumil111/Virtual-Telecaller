{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:28:03.424071Z",
     "iopub.status.busy": "2025-02-09T06:28:03.423752Z",
     "iopub.status.idle": "2025-02-09T06:28:03.430054Z",
     "shell.execute_reply": "2025-02-09T06:28:03.429118Z",
     "shell.execute_reply.started": "2025-02-09T06:28:03.424044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import nest_asyncio\n",
    "\n",
    "import io\n",
    "import pygame\n",
    "import edge_tts\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import speech_recognition as sr\n",
    "from langchain_groq  import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from typing import Literal, TypedDict, List\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:19:38.755316Z",
     "iopub.status.busy": "2025-02-09T06:19:38.754372Z",
     "iopub.status.idle": "2025-02-09T06:19:38.759194Z",
     "shell.execute_reply": "2025-02-09T06:19:38.758335Z",
     "shell.execute_reply.started": "2025-02-09T06:19:38.755274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['GROQ_Key'] = os.getenv(\"GROQ_API_KEY\") \n",
    "pygame.mixer.init()\n",
    "nest_asyncio.apply()\n",
    "MAX_CONCURRENT_TTS = 3\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_TTS)\n",
    "memory = MemorySaver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure folder exists \n",
    "\n",
    "os.makedirs(\"vectorstore\", exist_ok=True)\n",
    "os.makedirs(\"data\",exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:33:47.388200Z",
     "iopub.status.busy": "2025-02-09T06:33:47.387906Z",
     "iopub.status.idle": "2025-02-09T06:33:47.392279Z",
     "shell.execute_reply": "2025-02-09T06:33:47.391521Z",
     "shell.execute_reply.started": "2025-02-09T06:33:47.388176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages : list[BaseMessage]\n",
    "   \n",
    "    response : str # the response to the query\n",
    "    context_docs : List[str]  # the context documents\n",
    "    path : str # the path to the current state \n",
    "    history_docs : List[str] # the history documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:19:40.498064Z",
     "iopub.status.busy": "2025-02-09T06:19:40.497835Z",
     "iopub.status.idle": "2025-02-09T06:19:40.778023Z",
     "shell.execute_reply": "2025-02-09T06:19:40.777403Z",
     "shell.execute_reply.started": "2025-02-09T06:19:40.498045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='exit')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal['vectorstore', 'wiki_search', 'llm', 'exit'] = Field(\n",
    "        ..., description=\"Route query to appropriate source: 'vectorstore' for Nirma-related queries, 'wiki_search' for general knowledge, 'llm' for unrelated questions, and 'exit' for quitting.\"\n",
    "    )\n",
    "route_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"\"\"\n",
    "        You are an AI assistant responsible for routing user queries to the most appropriate source. You have access to:\n",
    "\n",
    "        1. **Nirma University Vectorstore** - Contains detailed information about Nirma University, including:\n",
    "           - Historical Background  \n",
    "           - Academic Programs  \n",
    "           - Fee Structure  \n",
    "           - Accreditations and Rankings  \n",
    "           - Placement Statistics  \n",
    "           - Internship Opportunities  \n",
    "           - Infrastructure and Facilities  \n",
    "           - Campus Life and Extracurricular Activities  \n",
    "           - Student Support Services  \n",
    "           - Alumni Network  \n",
    "\n",
    "        2. **Wikipedia Search (wiki_search)** - Use this for general knowledge questions that are **not related** to Nirma University.\n",
    "\n",
    "        3. **LLM** - Handle all **other queries** using LLM, including random, open-ended, or unclear questions.\n",
    "\n",
    "        4. **Exit** - If the user explicitly states they want to exit (e.g., \"exit,\" \"quit,\" \"end chat\"), return `\"exit\"`.\n",
    "\n",
    "        **Routing Rules:**\n",
    "        - If the query is **about Nirma University**, route it to **vectorstore**.\n",
    "        - If the query is **general factual knowledge**, use **wiki_search**.\n",
    "        - **All other queries, by default, should go to LLM**.\n",
    "        - If the query is **about exiting**, return `\"exit\"`.\n",
    "\n",
    "        Always ensure queries are routed efficiently and accurately.\n",
    "        \"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key = os.environ['GROQ_Key'],\n",
    "    model_name = \"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "router = route_prompt | llm\n",
    "\n",
    "router.invoke(\"Ok thankyou and See you later\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:19:42.081988Z",
     "iopub.status.busy": "2025-02-09T06:19:42.081742Z",
     "iopub.status.idle": "2025-02-09T06:19:42.086075Z",
     "shell.execute_reply": "2025-02-09T06:19:42.085317Z",
     "shell.execute_reply.started": "2025-02-09T06:19:42.081967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def route(state: State):\n",
    "    query = state['messages'][-1].content\n",
    "    source = router.invoke({'query': query})\n",
    "    if source.datasource.lower() == 'vectorstore':\n",
    "        return 'vectorstore'\n",
    "    elif source.datasource.lower() == 'wiki_search':\n",
    "        return 'wiki_search'\n",
    "    \n",
    "    elif source.datasource.lower() == 'llm':\n",
    "        return 'llm'\n",
    "    \n",
    "    elif source.datasource.lower() == 'exit':\n",
    "        return 'end'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:19:43.805253Z",
     "iopub.status.busy": "2025-02-09T06:19:43.804975Z",
     "iopub.status.idle": "2025-02-09T06:19:52.218356Z",
     "shell.execute_reply": "2025-02-09T06:19:52.217710Z",
     "shell.execute_reply.started": "2025-02-09T06:19:43.805230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index: <pinecone.db_data.index.Index object at 0x000001C1B451FA10>\n",
      "Uploaded 2 records to Pinecone ')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pinecone import Pinecone\n",
    "import uuid\n",
    "os.environ[\"PINECONE_API\"] = os.getenv(\"PINECONE_API\")\n",
    "os.environ[\"PINECONE_ENV\"] = os.getenv(\"PINECONE_ENV\")\n",
    "os.environ[\"PINECONE_HOST\"] = os.getenv(\"PINECONE_HOST\")\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API\"])\n",
    "\n",
    "index = pc.Index(\n",
    "    host=os.environ[\"PINECONE_HOST\"],\n",
    ")\n",
    "\n",
    "print(f\"Connected to Pinecone index: {index}\")\n",
    "# from langchain_chroma import Chroma\n",
    "loader = TextLoader('data/rag_data.txt',encoding='utf-8')\n",
    "data = loader.load()\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "\n",
    "for doc in data:\n",
    "    text+= doc.page_content + \"\\n\\n\"\n",
    "\n",
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \".\",\",\"]\n",
    "    )\n",
    "\n",
    "# docs = text_splitter.split_documents(data)\n",
    "data_chunks = text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "def upload_chunks_to_pinecone(chunks , namespace):\n",
    "    records = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        records.append(\n",
    "            {\n",
    "                \"_id\": str(uuid.uuid4()),\n",
    "                \"text\": chunk,  # <--- Use \"text\" instead of \"chunk_text\"\n",
    "                \"chunk_index\": i,  # optional metadata field\n",
    "            }\n",
    "        )\n",
    "\n",
    "    index.upsert_records(namespace =namespace,\n",
    "                         records=records)\n",
    "    print(f\"Uploaded {len(records)} records to Pinecone ')\")\n",
    "\n",
    "\n",
    "def get_top_k_similar(query: str, session_id: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    This function retrieves the top k similar documents from the vector database based on the query.\n",
    "    \"\"\"\n",
    "\n",
    "    results = index.search(\n",
    "        namespace=session_id,\n",
    "        query={\"inputs\": {\"text\": query}, \"top_k\": k},\n",
    "        fields=[\"text\"],\n",
    "    )\n",
    "\n",
    "    hits = results.get(\"result\", {}).get(\"hits\", [])\n",
    "\n",
    "    text = \"\"\n",
    "    unique_texts = set()\n",
    "    for hit in hits:\n",
    "        content = hit.get(\"fields\", {}).get(\"text\", \"\").strip()\n",
    "        if content and content not in unique_texts:\n",
    "            text += content + \"\\n\"\n",
    "            unique_texts.add(content)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "upload_chunks_to_pinecone(data_chunks, \"gromo-hackathon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:19:52.219954Z",
     "iopub.status.busy": "2025-02-09T06:19:52.219363Z",
     "iopub.status.idle": "2025-02-09T06:19:52.224195Z",
     "shell.execute_reply": "2025-02-09T06:19:52.223355Z",
     "shell.execute_reply.started": "2025-02-09T06:19:52.219930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_docs': ['Indiaâ€™s brightest AI developers, designers, and product thinkers to co-create solutions ',\n",
       "  'that can solve real, day-to-day problems faced by GroMo Partners. ',\n",
       "  'ðŸ§  General Guidelines for Participants ',\n",
       "  'To build a winning solution, we recommend you: ',\n",
       "  'â—\\u200b Download and explore the GroMo App from the Play Store to experience the platform ',\n",
       "  'firsthand.  ',\n",
       "  ' ',\n",
       "  'â—\\u200b Deep-dive into the mindset, behavior, and lifecycle of GroMo Partners (GPs) â€” ',\n",
       "  'Indiaâ€™s financial micro-entrepreneurs. ',\n",
       "  'â—\\u200b Identify real pain points that GPs face in scaling their monthly income â€” whether itâ€™s ',\n",
       "  'finding leads, converting them, or managing post-sale engagement etc. ',\n",
       "  'â—\\u200b Think AI-first â€” explore how artificial intelligence can be used to automate, guide, or ',\n",
       "  'amplify key actions for the GP. ',\n",
       "  'â—\\u200b Prioritize scalability and impact â€” build solutions that are contextual, earnings-linked, ',\n",
       "  'and capable of being deployed to thousands of GPs. ',\n",
       "  'â—\\u200b Keep the end user at the center â€” focus on simplicity, mobile-first usability, and',\n",
       "  'FinArva AI 2025 ',\n",
       "  'Reimagining the future of financial product distribution ',\n",
       "  'with AI ',\n",
       "  '*This document outlines the core problem statement and expectations for participants in the ',\n",
       "  'FinArva AI Hackathon launched by GroMo in partnership with Amazon Web Services. ',\n",
       "  'About GroMo ',\n",
       "  'Founded in 2019, GroMo is one of Indiaâ€™s fastest-growing agent-led platforms for distributing ',\n",
       "  'financial products. It enables individuals â€” from first-time entrepreneurs to experienced ',\n",
       "  'advisors â€” to sell insurance, credit cards, loans, savings account, demat account and ',\n",
       "  'investments to customers in their own networks. At its core, GroMo operates as a tech-enabled ',\n",
       "  'bridge between Indiaâ€™s leading financial institutions and its underpenetrated, high-potential ',\n",
       "  'markets, powered by a distributed agent network. The business model focuses on empowering ',\n",
       "  'individuals/agents (GroMo Partners or GPs) to earn commission-based income through a ',\n",
       "  'streamlined digital platform.',\n",
       "  'â—\\u200b What if post-sale engagement was streamlined â€” with AI responding to common ',\n",
       "  'customer queries (like claim status or document uploads), or even prompting GPs when ',\n",
       "  'a customer is due for a renewal or upsell? ',\n",
       "  'â—\\u200b And what if growth itself was data-driven â€” with AI identifying GPs at risk of stagnation, ',\n",
       "  'nudging them with custom playbooks to expand into new categories, or suggesting ',\n",
       "  'monthly income boosters based on market trends and personal activity? ',\n",
       "  'The possibilities are limitless. This isnâ€™t just about improving efficiency â€” itâ€™s about redefining ',\n",
       "  'the GP experience. With AI, every GP can have a personalized strategy, real-time support, and ',\n",
       "  'intelligent growth plans â€” helping them not only succeed, but thrive. With the right intent, talent, ',\n",
       "  'and execution, they can be built today.To unlock this future, GroMo is launching the GroMo AI Hackathon 2025 â€” a call to action for ',\n",
       "  'Indiaâ€™s brightest AI developers, designers, and product thinkers to co-create solutions',\n",
       "  ''],\n",
       " 'messages': [HumanMessage(content='Tell me about hackathon details and price money', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_docs(state: State):\n",
    "\n",
    "\n",
    "    query = state[\"messages\"][-1].content\n",
    "    session_id = \"gromo-hackathon\"  # Replace with your actual Pinecone namespace\n",
    "    top_k_text = get_top_k_similar(query, session_id, k=3)\n",
    "    context  = top_k_text.split(\"\\n\") if top_k_text else []\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # print(\"Reached RAG\")\n",
    "    # vector_store = FAISS.load_local(\n",
    "    #     \"vectorstore/rag_db\",\n",
    "    #     embeddings=embeddings,\n",
    "    #     allow_dangerous_deserialization=True,\n",
    "    # )\n",
    "    # retriever = vector_store.as_retriever()\n",
    "    # result = retriever.invoke(state[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # context = [doc.page_content for doc in result]\n",
    "    return {\"context_docs\": context, \"messages\": state[\"messages\"]}\n",
    "\n",
    "retrieve_docs({\n",
    "    \"messages\": [HumanMessage(content=\"Tell me about hackathon details and price money\")],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:19:52.226110Z",
     "iopub.status.busy": "2025-02-09T06:19:52.225904Z",
     "iopub.status.idle": "2025-02-09T06:19:52.453973Z",
     "shell.execute_reply": "2025-02-09T06:19:52.453362Z",
     "shell.execute_reply.started": "2025-02-09T06:19:52.226092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_result=1, doc_content=500)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "def wiki_search(state: State):\n",
    "    print(\"Reached Wiki\")\n",
    "    docs = wiki.invoke(state['messages'][-1].content)\n",
    "    return { 'context_docs' : [docs], 'messages': state['messages']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_query(state: State):\n",
    "    \n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an Helpful AI Assistant . Named NirmaBot , Your task is to talk with user in human like manner and help them with their queries.\"),\n",
    "        HumanMessage(content=state['messages'][-1].content)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Extract content from the response\n",
    "    response_text = response.content.split('</think>')[-1]\n",
    "    \n",
    "    return {'context_docs': [response_text], 'messages': state['messages']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_retriver(state: State):\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    print(\"Reached History\\n\")\n",
    "\n",
    "\n",
    "    query = state['messages'][-1].content\n",
    "    # bussiness_name = state['b']\n",
    "\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=os.environ[\"GROQ_Key\"], model_name=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    # Load existing history\n",
    "    try:\n",
    "    \n",
    "        with open(\"data/history.txt\",\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            history_data = f.read()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        history_data = \"\"\n",
    "\n",
    "\n",
    "    # Add the latest message to the history\n",
    "\n",
    "    for message in state['messages']:\n",
    "        if isinstance(message,HumanMessage):\n",
    "            history_data += f\"User :{message.content.strip()}\\n\"\n",
    "\n",
    "        else:\n",
    "            history_data += f\"AI_Bot :{message.content.strip()}\\n\"\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        You are an AI summarizer responsible for extracting and summarizing the most relevant topics from user and AI conversations. Your goal is to maintain a concise record of key discussion points based on frequency and importance.\n",
    "        ## History_Docs:  \n",
    "        This section contains a structured summary of past conversations. It is meant **only for reference** and should not be used unless relevant to the current discussion.\n",
    "        ### Summarization Guidelines:\n",
    "        - Prioritize recent and critical points first.\n",
    "        - Summarize in a clear, pointwise format.\n",
    "        - Keep it conciseâ€”only capture key information, avoiding unnecessary details.\n",
    "        ### Output Format:\n",
    "        \n",
    "        History_Docs: A structured, pointwise summary of key discussion topics.  .\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    history_text = system_prompt + history_data\n",
    "    \n",
    "\n",
    "    response = llm.invoke(history_text)\n",
    "\n",
    "    with open(\"data/history.txt\", \"w\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "\n",
    "        f.write(response.content)\n",
    "\n",
    "    loader = TextLoader(\"data/history.txt\")\n",
    "\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(data)\n",
    "\n",
    "    vector_store1 = FAISS.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "\n",
    "    vector_store1.save_local(\"vectorstore/history_db\")\n",
    "\n",
    "\n",
    "\n",
    "    vector_store = FAISS.load_local(\"vectorstore/history_db\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # create retriever according to the user query\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "\n",
    "\n",
    "    result = retriever.invoke(query)\n",
    "\n",
    "    history = [doc.page_content for doc in result]\n",
    "\n",
    "\n",
    "    return {'history_docs': history, 'messages': state['messages']}\n",
    "\n",
    "\n",
    "    #test the function\n",
    "\n",
    "\n",
    "print(history_retriver({'messages': [HumanMessage(content=\"What is nirma \")]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:33:32.459585Z",
     "iopub.status.busy": "2025-02-09T06:33:32.459156Z",
     "iopub.status.idle": "2025-02-09T06:33:32.569439Z",
     "shell.execute_reply": "2025-02-09T06:33:32.568743Z",
     "shell.execute_reply.started": "2025-02-09T06:33:32.459548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.environ[\"GROQ_Key\"],\n",
    "    # model_name = 'deepseek-r1-distill-llama-70b'\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        You are NirmaBot, an AI assistant designed to interact with users in an ultra-friendly, human-like manner. Your goal is to provide helpful, engaging, and natural responsesâ€”just like a real human assistant.\n",
    "\n",
    "        ### How to Respond:\n",
    "        - Prioritize context_docs (retrieved knowledge) for answering user queries accurately.\n",
    "        - Use history_docs **only if the current query relates to past discussions**.\n",
    "        - If a query is unrelated to history, do **not** use past information in your response.\n",
    "        \n",
    "\n",
    "        ### Guidelines for a Natural Tone:\n",
    "        âœ… Be warm & friendly - Sound like a helpful human, not a bot.  \n",
    "        âœ… Use casual phrasing - Like chatting with a friend or a helpful colleague.  \n",
    "\n",
    "        ### Important Rule:\n",
    "        - **Never use history_docs unless explicitly relevant to the user's current query.**  \n",
    "\n",
    "        Your main job? **Focus on the user's request at the moment** while maintaining natural and helpful conversations!\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"Previous History : \")\n",
    "\n",
    "    print(state[\"history_docs\"])\n",
    "    context = \" \".join(state[\"context_docs\"]).join(state[\"history_docs\"])\n",
    "\n",
    " \n",
    "\n",
    "    full_system_prompt = system_prompt + \" \" + context\n",
    "\n",
    "    \n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=full_system_prompt),\n",
    "        # *state[\"messages\"],\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"Reached LLM\")\n",
    "    response = llm.invoke(messages)\n",
    "    response = response.content.split(\"</think>\")[-1]\n",
    "    print(\"Response : \", response)\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=response)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "\n",
    "\n",
    "def speech_to_text(state: State):\n",
    "    # print(state)\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "\n",
    "        \n",
    "        while (True):\n",
    "            print(\"Listening... Speak now!\")\n",
    "            audio = recognizer.listen(source , phrase_time_limit=5) # listen for the first phrase and extract it into audio data\n",
    "            try:\n",
    "                query = recognizer.recognize_google(audio)\n",
    "                \n",
    "                print(f\"Query: {query}\")\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except sr.RequestError:\n",
    "                print(\"Could not request results; check your network connection\")\n",
    "            except sr.WaitTimeoutError:\n",
    "                print(\"Timeout; no speech detected\")\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "                break\n",
    "\n",
    "        messages = state.get('messages', [])\n",
    "        return {'messages': messages + [HumanMessage(content=query)]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def text_to_speech(state):\n",
    "\n",
    "    pygame.mixer.init()\n",
    "    \n",
    "    \"\"\"\n",
    "    LangGraph Node: Converts text to speech using Edge-TTS and plays it in real time.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): LangGraph state containing 'response' text.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The same state dictionary (to maintain LangGraph flow).\n",
    "    \"\"\"\n",
    "    text = state['response']\n",
    "    chunks = text.split(\". \")  # Split text into sentences\n",
    "\n",
    "    if not os.path.exists(\"audiochunks\"):\n",
    "        os.makedirs(\"audiochunks\")\n",
    "\n",
    "    async def generate_audio(text, index, audio_queue):\n",
    "        \"\"\"Generates speech from text using Edge-TTS and stores it in an async queue.\"\"\"\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        async with semaphore:\n",
    "            tts = edge_tts.Communicate(text, voice=\"en-US-JennyNeural\")\n",
    "            audio_stream = io.BytesIO()\n",
    "\n",
    "            async for chunk in tts.stream():\n",
    "                if chunk[\"type\"] == \"audio\":\n",
    "                    audio_stream.write(chunk[\"data\"])\n",
    "\n",
    "            \n",
    "            filename = f\"audiochunks/chunk_{index}.mp3\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(audio_stream.getvalue())\n",
    "\n",
    "            await audio_queue.put((index, filename))\n",
    "\n",
    "    async def play_audio(total_chunks, audio_queue):\n",
    "        \"\"\"Plays generated audio chunks strictly in the correct order using pygame.mixer and deletes files afterward.\"\"\"\n",
    "        expected_index = 0\n",
    "        ready_chunks = {}  # Dictionary to store chunks until they can be played in order\n",
    "        played_files = []  # Track files to delete later\n",
    "\n",
    "        while expected_index < total_chunks:\n",
    "            index, filename = await audio_queue.get()\n",
    "\n",
    "            # Store chunk for ordered playback\n",
    "            ready_chunks[index] = filename\n",
    "\n",
    "            # Ensure playback happens in the correct order\n",
    "            while expected_index in ready_chunks:\n",
    "                filename = ready_chunks.pop(expected_index)\n",
    "                print(f\"ðŸ”Š Playing chunk {expected_index}...\")\n",
    "\n",
    "                pygame.mixer.music.load(filename)\n",
    "                pygame.mixer.music.play()\n",
    "\n",
    "                while pygame.mixer.music.get_busy():\n",
    "                    await asyncio.sleep(0.02)\n",
    "\n",
    "                played_files.append(filename)  # Track files for deletion\n",
    "                expected_index += 1  # Move to next chunk\n",
    "\n",
    "\n",
    "        \n",
    "        # pygame.mixer.music.stop() # Stop playback after all chunks are played\n",
    "        pygame.mixer.quit()  # Clean up pygame resources\n",
    "        pygame.quit()\n",
    "        \n",
    "        \n",
    "        print(\"âœ… All chunks played in the correct order. Deleting files...\")\n",
    "\n",
    "\n",
    "\n",
    "        # Delete all played audio files\n",
    "        for file in played_files:\n",
    "\n",
    "            try:\n",
    "                os.remove(file)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "\n",
    "    async def process_chunks():\n",
    "        \"\"\"Manages TTS generation and playback.\"\"\"\n",
    "        audio_queue = asyncio.PriorityQueue()\n",
    "        tasks = [generate_audio(chunk, idx, audio_queue) for idx, chunk in enumerate(chunks)]\n",
    "        await asyncio.gather(*tasks, play_audio(len(chunks), audio_queue))\n",
    "\n",
    "    # Run async function inside a sync wrapper\n",
    "    asyncio.run(process_chunks())\n",
    "    \n",
    "    return {'response': state['response'], 'messages': state['messages']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:33:57.185384Z",
     "iopub.status.busy": "2025-02-09T06:33:57.185018Z",
     "iopub.status.idle": "2025-02-09T06:33:57.203301Z",
     "shell.execute_reply": "2025-02-09T06:33:57.202551Z",
     "shell.execute_reply.started": "2025-02-09T06:33:57.185350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node('stt', speech_to_text)\n",
    "builder.add_node('wiki_search', wiki_search)\n",
    "builder.add_node('RAG', retrieve_docs)\n",
    "builder.add_node('llm', llm_query)\n",
    "builder.add_node('chatbot', chatbot)\n",
    "builder.add_node('tts', text_to_speech)\n",
    "\n",
    "builder.add_node('history', history_retriver)\n",
    "\n",
    "builder.add_edge(START, 'stt')\n",
    "builder.add_conditional_edges(\n",
    "    'stt', \n",
    "    route,\n",
    "    {\n",
    "        'wiki_search': 'wiki_search',\n",
    "        'vectorstore': 'RAG',\n",
    "        'llm': 'llm',\n",
    "        'end': END,\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_edge('RAG', 'history')\n",
    "builder.add_edge('wiki_search', 'history')\n",
    "builder.add_edge('llm', 'history')\n",
    "builder.add_edge('history', 'chatbot')\n",
    "\n",
    "\n",
    "builder.add_edge('chatbot', 'tts')\n",
    "builder.add_edge('tts',\"stt\")\n",
    "\n",
    "# graph = builder.compile(checkpointer=memory)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:33:59.291261Z",
     "iopub.status.busy": "2025-02-09T06:33:59.290916Z",
     "iopub.status.idle": "2025-02-09T06:33:59.295404Z",
     "shell.execute_reply": "2025-02-09T06:33:59.294574Z",
     "shell.execute_reply.started": "2025-02-09T06:33:59.291231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T06:34:00.236334Z",
     "iopub.status.busy": "2025-02-09T06:34:00.236036Z",
     "iopub.status.idle": "2025-02-09T06:34:09.851961Z",
     "shell.execute_reply": "2025-02-09T06:34:09.851011Z",
     "shell.execute_reply.started": "2025-02-09T06:34:00.236308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "response = graph.invoke({'query': ''}, config=config, stream_mode='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6628610,
     "sourceId": 10696971,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6630768,
     "sourceId": 10700214,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
